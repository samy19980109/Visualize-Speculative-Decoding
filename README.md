<div align="center">

# âš¡ SpeculatoViz

### Real-time visualization of speculative decoding at the edge-cloud boundary

[![Cerebras](https://img.shields.io/badge/Target-Cerebras%20Wafer--Scale-FF6B00?style=for-the-badge&logo=lightning)](https://cerebras.ai)
[![MLX](https://img.shields.io/badge/Draft-MLX--LM%20on%20Apple%20Silicon-0071E3?style=for-the-badge&logo=apple)](https://github.com/ml-explore/mlx-lm)
[![React](https://img.shields.io/badge/Frontend-React%2019%20%2B%20D3.js-61DAFB?style=for-the-badge&logo=react)](https://react.dev)
[![FastAPI](https://img.shields.io/badge/Backend-FastAPI%20WebSocket-009688?style=for-the-badge&logo=fastapi)](https://fastapi.tiangolo.com)

---

**A 16GB MacBook Air drafts tokens locally at 50-200 tok/s on Apple's Neural Engine.**
**Cerebras verifies an entire batch in one API call at 1000+ tok/s.**
**The result: 3-4x faster inference than autoregressive decoding, with zero quality loss.**

*This project makes every step of that process visible in real time.*

---

[The Idea](#-the-core-idea) Â· [How It Works](#-how-speculative-decoding-works) Â· [Architecture](#-system-architecture) Â· [Visualizations](#-what-you-see) Â· [Quick Start](#-quick-start) Â· [Benchmarks](#-benchmarks) Â· [Deep Dive](#-technical-deep-dive)

</div>

---

## ğŸ§  The Core Idea

Large language models generate text one token at a time. Each token requires a full forward pass through the model â€” and when the model lives in the cloud, each token also requires a full network round trip. For a 70B-parameter model, that means:

```
Traditional autoregressive:  Tokenâ‚ â†’ wait â†’ Tokenâ‚‚ â†’ wait â†’ Tokenâ‚ƒ â†’ wait â†’ ...
                             Each token = 1 API call = 1 network round trip
                             512 tokens Ã— 30ms/call = 15.4 seconds
```

**Speculative decoding flips this on its head.** Instead of asking the cloud for one token at a time, we:

1. Run a *tiny* model locally (3B params, fits in 1.8GB) to **draft** K tokens ahead
2. Send all K drafts to the cloud in a **single** batched verification call
3. The cloud model checks all K tokens at once â€” accepting correct ones, fixing incorrect ones
4. We produce 1 to K+1 tokens per round trip instead of 1

```
Speculative decoding:  [Draft K=8 locally in 20ms] â†’ [Verify all 8 in one 50ms API call]
                       â†’ Accept 5-6 tokens per round
                       â†’ 512 tokens Ã· 5.5 tokens/round Ã— 70ms/round = 6.5 seconds
                       â†’ 2.4x speedup. Zero quality loss.
```

### Why Cerebras Makes This Work

Speculative decoding has a critical dependency: the target model must verify K tokens **as fast as** it would generate 1 token. Traditional GPU clouds struggle with this â€” batch scheduling adds latency variance, and verification time grows with K.

Cerebras' wafer-scale architecture is uniquely positioned:

| Property | Traditional GPU Cloud | Cerebras Wafer-Scale |
|----------|:--------------------:|:--------------------:|
| **Latency variance** | High (scheduling noise, queuing) | **Deterministic** |
| **Verify K tokens vs 1** | Latency grows with K | **O(1) â€” same latency** |
| **Speedup ceiling** | ~2-3x (latency-bound) | **~4-8x (compute-bound)** |
| **Cost scaling** | O(K) per verification | **O(1) per verification** |
| **Deeper speculation (K=8+)** | Diminishing returns | **Linear gains** |

**The key equation:**

```
                         K
Speedup = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          1 + Î» Ã— (RTT / T_draft)

With Cerebras:
  T_verify(K tokens) â‰ˆ T_verify(1 token)  â† wafer-scale parallelism
  â†’ Speedup approaches K as RTT decreases
  â†’ K=8 with 65% acceptance rate â†’ 5.2 tokens/round â†’ 3.4x speedup
```

With Cerebras' predictable, ultra-low latency, speculative decoding becomes a **"set and forget" optimization** rather than a fragile heuristic that only works under ideal conditions.

---

## ğŸ”„ How Speculative Decoding Works

SpeculatoViz implements the **modified rejection sampling** algorithm from [Leviathan et al. 2023](https://arxiv.org/abs/2211.17192), with real-time visualization of every step.

### The Five-Phase Loop

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ROUND n                                                            â”‚
â”‚                                                                     â”‚
â”‚  â‘  DRAFT         Local MLX model generates K candidate tokens       â”‚
â”‚     10-50ms      Each token comes with log-probabilities q(x)       â”‚
â”‚                                                                     â”‚
â”‚  â‘¡ VERIFY        Single batched API call to Cerebras                â”‚
â”‚     30-100ms     Returns log-probabilities p(x) for all K+1 pos    â”‚
â”‚                                                                     â”‚
â”‚  â‘¢ COMPARE       For each position i = 0, 1, ..., K-1:             â”‚
â”‚     <1ms           Sample u ~ Uniform(0,1)                          â”‚
â”‚                    If u < min(1, p(xáµ¢)/q(xáµ¢)) â†’ âœ… ACCEPT          â”‚
â”‚                    Else â†’ âŒ REJECT, resample from max(0, p-q)       â”‚
â”‚                    Stop at first rejection                           â”‚
â”‚                                                                     â”‚
â”‚  â‘£ BONUS         If all K tokens accepted:                          â”‚
â”‚                    Extract K+1th token from target â†’ ğŸ BONUS        â”‚
â”‚                    This round produced K+1 tokens!                   â”‚
â”‚                                                                     â”‚
â”‚  â‘¤ EMIT          Stream events to frontend via WebSocket            â”‚
â”‚     50-80ms      DraftToken â†’ VerifyResult â†’ Metrics per round      â”‚
â”‚     stagger                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Token Outcomes (Color-Coded in the Visualization)

| Status | Color | Meaning |
|--------|:-----:|---------|
| **Accepted** | ğŸŸ¢ Green | Draft token matches target distribution â€” kept as-is |
| **Rejected** | ğŸ”´ Red | Draft token diverged too far â€” discarded |
| **Resampled** | ğŸŸ  Orange | Rejected token replaced by sampling from `max(0, p - q)` |
| **Bonus** | ğŸ”µ Blue | Free extra token when all K drafts were accepted |
| **Pending** | âšª Gray | Awaiting verification (visible during the draft phase) |

### The Mathematical Guarantee

Modified rejection sampling ensures the output distribution **exactly matches** what the target model would have produced autoregressively. This is not an approximation â€” it is a mathematically proven distribution-preserving transform:

```
For each draft token xáµ¢ drawn from draft distribution q:

    acceptance_probability = min(1, p(xáµ¢) / q(xáµ¢))

    If accepted:  output xáµ¢                    (same as target would produce)
    If rejected:  sample from norm(max(0, p-q)) (corrects the distribution)

Result: P(output) â‰¡ P(target model output)   âˆ€ inputs, temperatures, sequences
```

The better the draft model approximates the target, the higher the acceptance rate, and the greater the speedup â€” but output quality is **always** identical to the target model alone.

---

## ğŸ— System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            EDGE DEVICE (Your Mac)                            â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         WebSocket          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   FRONTEND            â”‚ â—„â”€â”€â”€â”€ JSON events â”€â”€â”€â”€â–º    â”‚   BACKEND         â”‚  â”‚
â”‚  â”‚   React 19 + Vite     â”‚    (auto snakeâ†”camel)      â”‚   FastAPI + ASGI  â”‚  â”‚
â”‚  â”‚                       â”‚                            â”‚                   â”‚  â”‚
â”‚  â”‚  â”Œâ”€ TokenTree.tsx     â”‚    /ws/tokens              â”‚  â”Œâ”€ main.py      â”‚  â”‚
â”‚  â”‚  â”‚  D3.js force graph â”‚    Real-time streaming     â”‚  â”‚  Entry + WS    â”‚  â”‚
â”‚  â”‚  â”‚                    â”‚                            â”‚  â”‚                â”‚  â”‚
â”‚  â”‚  â”œâ”€ TextOutput.tsx    â”‚    /api/health             â”‚  â”œâ”€ speculator.py â”‚  â”‚
â”‚  â”‚  â”‚  Color-coded text  â”‚    Health check            â”‚  â”‚  Orchestrator  â”‚  â”‚
â”‚  â”‚  â”‚                    â”‚                            â”‚  â”‚                â”‚  â”‚
â”‚  â”‚  â”œâ”€ KPIDashboard.tsx  â”‚                            â”‚  â”œâ”€ rejection_    â”‚  â”‚
â”‚  â”‚  â”‚  â”œ AcceptanceGauge â”‚                            â”‚  â”‚  sampling.py   â”‚  â”‚
â”‚  â”‚  â”‚  â”œ SpeedupIndicatorâ”‚                            â”‚  â”‚                â”‚  â”‚
â”‚  â”‚  â”‚  â”œ TPSChart        â”‚                            â”‚  â”œâ”€ metrics.py   â”‚  â”‚
â”‚  â”‚  â”‚  â”” LatencyChart    â”‚                            â”‚  â”‚  50-round avg  â”‚  â”‚
â”‚  â”‚  â”‚                    â”‚                            â”‚  â”‚                â”‚  â”‚
â”‚  â”‚  â””â”€ PromptInput.tsx   â”‚                            â”‚  â”œâ”€ schemas.py   â”‚  â”‚
â”‚  â”‚     K, temp, tokens   â”‚                            â”‚  â”‚  Pydantic      â”‚  â”‚
â”‚  â”‚                       â”‚                            â”‚  â”‚                â”‚  â”‚
â”‚  â”‚  Hooks:               â”‚                            â”‚  â”œâ”€ interfaces.pyâ”‚  â”‚
â”‚  â”‚  â”œ useWebSocket       â”‚                            â”‚  â”‚  DI protocols  â”‚  â”‚
â”‚  â”‚  â”” useSpecDecState    â”‚                            â”‚  â”‚                â”‚  â”‚
â”‚  â”‚                       â”‚                            â”‚  â””â”€ config.py    â”‚  â”‚
â”‚  â”‚  Lib:                 â”‚                            â”‚     .env + cache  â”‚  â”‚
â”‚  â”‚  â”œ camelCase.ts       â”‚                            â”‚                   â”‚  â”‚
â”‚  â”‚  â”œ styles.ts          â”‚                            â”‚  Tests:           â”‚  â”‚
â”‚  â”‚  â”” treeUtils.ts       â”‚                            â”‚  â”” test_speculatorâ”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚            â”‚
â”‚                                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚                                                    â–¼                       â–¼ â”‚
â”‚                                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚                                       â”‚  draft_model.py  â”‚   â”‚ target_     â”‚ â”‚
â”‚                                       â”‚                  â”‚   â”‚ model.py    â”‚ â”‚
â”‚                                       â”‚  MLX-LM local    â”‚   â”‚             â”‚ â”‚
â”‚                                       â”‚  Apple Neural    â”‚   â”‚  Cerebras   â”‚ â”‚
â”‚                                       â”‚  Engine          â”‚â”€â”€â”€â”‚  Cloud API  â”‚ â”‚
â”‚                                       â”‚                  â”‚   â”‚             â”‚ â”‚
â”‚                                       â”‚  Llama 3.2 3B    â”‚   â”‚  GPT-OSS    â”‚ â”‚
â”‚                                       â”‚  4-bit quantized â”‚   â”‚  120B       â”‚ â”‚
â”‚                                       â”‚  ~1.8GB RAM      â”‚   â”‚  /v1/compl  â”‚ â”‚
â”‚                                       â”‚  50-200 tok/s    â”‚   â”‚  1000+ t/s  â”‚ â”‚
â”‚                                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow â€” One Complete Round

```
User types prompt
    â”‚
    â–¼
[WebSocket] â†’ StartGenerationRequest {prompt, k=8, temperature=0.7, maxTokens=256}
    â”‚
    â–¼
[Speculator] â”€â”€â”€ Phase 1: Draft â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â”‚   Tokenize prompt via chat template
    â”‚   Feed context_ids + generated_token_ids to MLX model
    â”‚   Call generate_step() K times with prompt cache
    â”‚   For each token: extract logprobs, entropy, top-10 alternatives
    â”‚   Emit DraftTokenEvent Ã— K (50ms stagger for animation)
    â”‚
    â”‚â”€â”€ Phase 2: Verify â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â”‚   Build full prompt text: chat_template(prompt) + generated_text
    â”‚   Single POST to Cerebras /v1/completions
    â”‚     â†’ logprobs=20 (top-20 per position), max_tokens=K+1
    â”‚   Parse K+1 TargetTokenInfo with logprobs + entropy
    â”‚
    â”‚â”€â”€ Phase 3: Compare â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â”‚   rejection_sampling.compare_tokens(draft_tokens, target_infos)
    â”‚   For each position: compute acceptance probability, accept/reject
    â”‚   Stop at first rejection (can't accept later tokens)
    â”‚   If all K accepted â†’ extract bonus token from position K+1
    â”‚
    â”‚â”€â”€ Phase 4: Update â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â”‚   Append accepted/resampled/bonus token IDs to generated_token_ids
    â”‚   Reconstruct generated_text from token IDs (avoids tokenizer drift)
    â”‚   Emit VerifyResultEvent per token (80ms stagger)
    â”‚   Check for EOS tokens (Llama + Harmony formats)
    â”‚
    â”‚â”€â”€ Phase 5: Metrics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â”‚   Record RoundStats into MetricsTracker (50-round rolling window)
    â”‚   Compute: acceptance_rate, effective_tps, speedup, latency breakdown
    â”‚   Emit MetricsEvent
    â”‚
    â–¼
[Frontend] â”€â”€â”€ useSpecDecState reducer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â”‚   DRAFT_TOKEN â†’ Build tree node, add to round chain
    â”‚   VERIFY_RESULT â†’ Update node status, color, rebuild generated text
    â”‚   METRICS_UPDATE â†’ Append to history for time-series charts
    â”‚   GENERATION_DONE â†’ Display final stats
    â”‚
    â–¼
[Rendered] â†’ Token tree animates, text streams in color, charts update live
```

---

## ğŸ¨ What You See

SpeculatoViz renders every step of the speculative decoding process across four synchronized panels:

### 1. Token Decision Tree (D3.js)

A force-directed graph where **every token is a node** and every round is a branch:

- **Node color** = token status (green/red/orange/blue â€” accepted/rejected/resampled/bonus)
- **Node size** = Shannon entropy of the token distribution (high entropy = large node = model was uncertain)
- **Node opacity** = acceptance probability (faint = barely accepted, solid = high confidence)
- **Edges** = sequential dependencies; dashed red edges show rejection points

Each round adds a new branch to the tree. You can watch the model speculate, see which tokens survive verification, and observe how the tree grows as generation progresses.

### 2. Streaming Text Output

The generated text appears token-by-token, color-coded by how each token was produced:

- **Green text** = accepted (draft model got it right)
- **Orange text** = resampled (draft was wrong, target model corrected it)
- **Blue text** = bonus (free extra token from a perfect round)

Hover over any token to see a tooltip with:
- Round and position within the speculation window
- Draft log-probability and target log-probability
- Acceptance probability as a percentage
- Shannon entropy of the distribution

### 3. Performance Dashboard

Four real-time charts updated every round:

| Panel | Chart Type | What It Shows |
|-------|-----------|---------------|
| **Acceptance Gauge** | Donut chart | Rolling acceptance rate with color coding (red <50%, yellow 50-75%, green >75%) |
| **Speedup Indicator** | Large number | Effective speedup vs autoregressive baseline (e.g., "3.4x") |
| **TPS Chart** | Area + line | Effective tokens/sec (blue) vs estimated autoregressive baseline (red dashed) |
| **Latency Chart** | Stacked bar | Draft latency (green) vs verification latency (amber) per round, last 10 rounds |

### 4. Interactive Controls

Tune the speculation parameters in real time:

- **Speculation depth K** (1-16): How many tokens to draft per round. Higher K = more aggressive speculation
- **Temperature** (0-2): Sampling temperature. Lower = more predictable = higher acceptance rate
- **Max tokens** (64-1024): Total generation length
- **Connection status**: Live indicator showing backend availability

---

## ğŸš€ Quick Start

### Prerequisites

| Requirement | Details |
|------------|---------|
| **Hardware** | Apple Silicon Mac (M1/M2/M3/M4) â€” required for MLX inference |
| **RAM** | 16GB recommended (8GB works with 1B draft model) |
| **Python** | 3.11 or later |
| **Node.js** | 18 or later |
| **Cerebras API Key** | Free at [cloud.cerebras.ai](https://cloud.cerebras.ai) |

### Installation

```bash
# 1. Clone the repository
git clone https://github.com/anthropics/visualize_speculative_decoding.git
cd visualize_speculative_decoding

# 2. Set up Python environment
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

# 3. Configure environment
cp .env.example .env
# Edit .env â€” add your CEREBRAS_API_KEY

# 4. Install frontend dependencies
cd frontend && npm install && cd ..
```

### Running

```bash
# Terminal 1 â€” Backend (auto-reloads on changes)
source .venv/bin/activate
uvicorn backend.main:app --reload
# â†’ Backend running at http://localhost:8000

# Terminal 2 â€” Frontend (hot module replacement)
cd frontend
npm run dev
# â†’ Frontend running at http://localhost:5173
#   (WebSocket and API calls proxied to :8000 automatically)
```

Open **http://localhost:5173**, type a prompt, and watch speculative decoding in action.

### Verify Setup

```bash
# Check backend health
curl http://localhost:8000/api/health
# â†’ {"status": "ok", "draft_model": "...", "target_model": "gpt-oss-120b", "draft_loaded": true}

# Test draft model independently
curl http://localhost:8000/api/test-draft
# â†’ Generates a few tokens to verify MLX is working

# Run unit tests (no GPU/MLX required â€” uses stub models)
pytest backend/tests/ -v
```

---

## âš™ï¸ Configuration

All settings are managed via environment variables (`.env` file):

```env
# â”€â”€â”€ Required â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CEREBRAS_API_KEY=your-api-key-here
CEREBRAS_TARGET_MODEL=gpt-oss-120b

# â”€â”€â”€ Speculation Parameters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DRAFT_MODEL=mlx-community/Llama-3.2-3B-Instruct-4bit
SPECULATION_K=8              # Tokens to draft per round (1-16)
TEMPERATURE=0.7              # Sampling temperature (0-2)
MAX_TOKENS=512               # Max tokens to generate (1-4096)
```

### Target Model Options

| Model | Parameters | Best For | Verification Speed |
|-------|:----------:|----------|:------------------:|
| `gpt-oss-120b` | 120B | **Best quality/speed tradeoff** | âš¡âš¡âš¡ |
| `qwen-3-32b` | 32B | Code generation, reasoning | âš¡âš¡âš¡ |

### Draft Model Options

| Model | Params | Disk | RAM | Speed | Acceptance Rate | Best For |
|-------|:------:|:----:|:---:|:-----:|:---------------:|----------|
| `Llama-3.2-3B-Instruct-4bit` | 3B | 1.8GB | ~4GB | âš¡âš¡âš¡ | â­â­â­â­ | **Recommended default** |
| `Llama-3.2-1B-Instruct-4bit` | 1B | 695MB | ~2GB | âš¡âš¡âš¡âš¡ | â­â­â­ | Quick tests, 8GB Macs |
| `Qwen2.5-3B-Instruct-4bit` | 3B | 1.7GB | ~4GB | âš¡âš¡âš¡ | â­â­â­â­â­ | Code, reasoning tasks |

All draft models use MLX 4-bit quantization and run on Apple's Neural Engine â€” no GPU required, no CUDA, no cloud costs for drafting.

---

## ğŸ“Š Benchmarks

Tested on MacBook Air M2 (16GB) â†’ Cerebras Cloud:

| Configuration | Effective TPS | Speedup | Avg Accepted/Round | Acceptance Rate |
|:-------------|:-------------:|:-------:|:-------------------:|:---------------:|
| Autoregressive baseline | 28 | 1.0x | â€” | â€” |
| Speculative K=4, 1B draft | 67 | **2.4x** | 2.8 | 70% |
| Speculative K=8, 3B draft | 94 | **3.4x** | 5.2 | 65% |
| Speculative K=8, 3B draft, code | 112 | **4.0x** | 6.1 | 76% |

### What Affects Performance

| Factor | Impact on Speedup | Why |
|--------|:-----------------:|-----|
| **Temperature â†“** | â†‘ Higher | Lower temp = more deterministic = draft and target agree more often |
| **Speculation depth K â†‘** | â†‘ Higher (with Cerebras) | More tokens per round; Cerebras verifies K tokens in O(1) |
| **Draft model quality â†‘** | â†‘ Higher | Better draft = higher acceptance rate = more tokens kept per round |
| **Domain specificity** | â†‘ Higher for code/structured | Predictable patterns (code syntax, JSON) have high acceptance |
| **Network latency â†‘** | â†“ Lower | More time per verification round = fewer rounds per second |

### Latency Breakdown (per round)

| Phase | Duration | Runs On |
|-------|:--------:|---------|
| Draft (K=8 tokens) | 10-50ms | Apple Neural Engine (local) |
| Verification (K+1 tokens) | 30-100ms | Cerebras Cloud (single API call) |
| Rejection sampling | <1ms | CPU (local) |
| Event emission (staggered) | 50-80ms | WebSocket (animation timing) |
| **Total round** | **~130-200ms** | |

---

## ğŸ”¬ Technical Deep Dive

### Key Design Decisions

#### 1. Completions API, Not Chat API

Cerebras does not support assistant message prefilling in its chat completions endpoint. To enable batch verification (sending K draft tokens and getting logprobs for all of them), we use the raw `/v1/completions` endpoint with the full prompt text:

```python
# target_model.py â€” How we verify draft tokens
response = await self.client.completions.create(
    model=self.model,
    prompt=full_prompt_text,      # chat_template(prompt) + generated_text_so_far
    max_tokens=k + 1,             # verify K drafts + 1 potential bonus
    logprobs=20,                  # top-20 alternatives per position
    temperature=0.01,             # near-greedy (>0 required for logprobs)
)
```

This design means the target model sees the exact same context the draft model used, ensuring valid comparison for rejection sampling.

#### 2. Token ID Tracking (Eliminating Tokenizer Drift)

A subtle but critical issue: naively concatenating token strings (`"Hello" + " world"`) can produce different tokenizations than encoding the full text at once. Over hundreds of tokens, this drift causes garbled output.

Our solution: accumulate **token IDs** and decode the full sequence each round:

```python
# speculator.py â€” How we maintain text consistency
self.generated_token_ids.extend([tok.token_id for tok in accepted_tokens])
self.generated_text_so_far = self._draft_model.decode(self.generated_token_ids)
# â†’ Always consistent with what the tokenizer would produce for the full text
```

#### 3. Log-Softmax Normalization

MLX's `generate_step()` returns raw logits, not normalized log-probabilities. Without normalization, rejection sampling produces incorrect acceptance probabilities:

```python
# draft_model.py â€” Normalizing logits to log-probabilities
logprobs_arr = logits.astype(mx.float32)
logprobs_arr = logprobs_arr - mx.logsumexp(logprobs_arr, keepdims=True)  # log-softmax
# â†’ Now p(x) = exp(logprobs_arr[x]) is a valid probability distribution
```

#### 4. Entropy as a Visual Dimension

Shannon entropy quantifies model uncertainty. We map it to node radius in the D3 tree:

```
H(p) = -Î£ p(x) log p(x)

Low entropy (H â‰ˆ 0)  â†’ small node â†’ model is confident â†’ likely accepted
High entropy (H â‰ˆ 4) â†’ large node â†’ model is uncertain â†’ likely rejected
```

This gives an immediate visual intuition: a tree full of small green nodes means the draft model is well-aligned with the target. Large red nodes signal disagreement.

#### 5. Rolling-Window Metrics

Metrics are computed over a sliding window of the last 50 rounds, not the entire generation:

```python
# metrics.py â€” Why windowed metrics matter
# Early rounds often have cold-start latency (model loading, cache warming)
# Windowed metrics reflect current steady-state performance
acceptance_rate = sum(r.accepted for r in window) / sum(r.total for r in window)
effective_tps = sum(r.tokens_produced for r in window) / sum(r.round_time_ms for r in window) * 1000
baseline_tps = 1000 / avg_verify_ms  # estimated autoregressive performance
speedup = effective_tps / baseline_tps
```

### Frontend Architecture

The frontend follows a clean unidirectional data flow:

```
WebSocket events â†’ useWebSocket hook (auto snakeâ†”camel conversion)
    â†’ useSpecDecState reducer (builds tree, accumulates text, tracks metrics)
        â†’ React components re-render
            â†’ D3 tree animates (CSS transitions, 500ms)
            â†’ Text streams with color coding
            â†’ Charts update with new data points
```

**State management** uses `useReducer` with five action types matching the five event types from the backend. The reducer builds a hierarchical tree structure from flat events â€” each round creates a branch, each draft token appends to the chain, and verification results update node colors and statuses. Dead state fields (`currentRound`, `error`, `finalStats`) have been removed; the reducer now tracks `acceptedTokens` incrementally and `finalGeneratedText` from the done event for authoritative text display.

**Tree construction** handles several edge cases:
- React StrictMode double-invocation (guarded by `findNode()` deduplication in `lib/treeUtils.ts`)
- Bonus tokens without explicit positions (fall back to deepest node via `findDeepest()` in `lib/treeUtils.ts`)
- Surgical spine-copy via `cloneRootWithUpdate()` â€” only clones nodes on the path from root to the updated node, leaving all other subtrees shared with the previous state (replaces full `structuredClone()`)
- Memoized `visibleTokens` filtering via `useMemo` to avoid recomputation on every render

---

## ğŸ› Project Structure

```
visualize_speculative_decoding/
â”‚
â”œâ”€â”€ backend/                    # Python â€” FastAPI + MLX + Cerebras
â”‚   â”œâ”€â”€ main.py                 # App entry, WebSocket endpoint, health checks
â”‚   â”œâ”€â”€ speculator.py           # Core orchestration loop (draft â†’ verify â†’ sample)
â”‚   â”œâ”€â”€ draft_model.py          # MLX-LM wrapper, local token generation
â”‚   â”œâ”€â”€ target_model.py         # Cerebras API client, batch verification
â”‚   â”œâ”€â”€ rejection_sampling.py   # Modified rejection sampling (Leviathan et al.)
â”‚   â”œâ”€â”€ interfaces.py           # Protocol definitions for dependency injection
â”‚   â”œâ”€â”€ metrics.py              # Rolling-window KPI tracker
â”‚   â”œâ”€â”€ schemas.py              # Pydantic event models + token status enum
â”‚   â”œâ”€â”€ config.py               # Environment variable loading + lru_cache singleton
â”‚   â””â”€â”€ tests/
â”‚       â””â”€â”€ test_speculator.py  # Unit tests with stub models (no GPU required)
â”‚
â”œâ”€â”€ frontend/                   # TypeScript â€” React 19 + Vite
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ App.tsx              # Root component, state + WebSocket coordination
â”‚       â”œâ”€â”€ types/index.ts       # Full type definitions mirroring backend schemas
â”‚       â”œâ”€â”€ hooks/
â”‚       â”‚   â”œâ”€â”€ useWebSocket.ts  # WebSocket with auto-reconnect + case conversion
â”‚       â”‚   â””â”€â”€ useSpecDecState.ts  # useReducer state machine, tree builder
â”‚       â”œâ”€â”€ components/
â”‚       â”‚   â”œâ”€â”€ Layout.tsx       # 2Ã—2 grid, dark theme
â”‚       â”‚   â”œâ”€â”€ PromptInput.tsx  # Input form with parameter sliders
â”‚       â”‚   â”œâ”€â”€ TokenTree.tsx    # D3 hierarchical token tree
â”‚       â”‚   â”œâ”€â”€ TextOutput.tsx   # Color-coded streaming text
â”‚       â”‚   â”œâ”€â”€ KPIDashboard.tsx # 2Ã—2 metrics grid container
â”‚       â”‚   â”œâ”€â”€ AcceptanceGauge.tsx   # Donut chart
â”‚       â”‚   â”œâ”€â”€ SpeedupIndicator.tsx  # Large speedup number
â”‚       â”‚   â”œâ”€â”€ TPSChart.tsx     # Area + line throughput chart
â”‚       â”‚   â””â”€â”€ LatencyChart.tsx # Stacked bar latency breakdown
â”‚       â””â”€â”€ lib/
â”‚           â”œâ”€â”€ colors.ts        # Consistent color palette + mapping functions
â”‚           â”œâ”€â”€ treeLayout.ts    # D3 tree layout computation
â”‚           â”œâ”€â”€ camelCase.ts     # Snakeâ†’camelCase recursive converter
â”‚           â”œâ”€â”€ styles.ts        # Shared CSS-in-JS constants for charts
â”‚           â””â”€â”€ treeUtils.ts     # Tree traversal helpers (findNode, findDeepest)
â”‚
â”œâ”€â”€ .env.example                # Environment variable template
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ pyproject.toml              # Project metadata + dependency spec
â””â”€â”€ CLAUDE.md                   # AI assistant context
```

---

## ğŸ“¡ WebSocket Protocol

The frontend and backend communicate via a structured JSON event protocol over WebSocket:

### Client â†’ Server

```json
{
  "prompt": "Explain quantum computing",
  "max_tokens": 256,
  "temperature": 0.7,
  "k": 8
}
```

### Server â†’ Client

**Draft Token Event** (emitted K times per round, 50ms apart):
```json
{
  "type": "draft_token",
  "round": 1,
  "position": 0,
  "token": "Quantum",
  "token_id": 34523,
  "logprob": -0.234,
  "entropy": 1.82,
  "top_tokens": [
    {"token": "Quantum", "logprob": -0.234},
    {"token": "The", "logprob": -1.567},
    ...
  ],
  "draft_time_ms": 12.4
}
```

**Verify Result Event** (emitted per token after verification, 80ms apart):
```json
{
  "type": "verify_result",
  "round": 1,
  "position": 0,
  "status": "accepted",
  "draft_token": "Quantum",
  "final_token": "Quantum",
  "draft_logprob": -0.234,
  "target_logprob": -0.198,
  "acceptance_prob": 1.0,
  "entropy": 1.65,
  "verify_latency_ms": 45.2
}
```

**Metrics Event** (emitted once per round):
```json
{
  "type": "metrics",
  "round": 1,
  "acceptance_rate": 0.75,
  "effective_tps": 94.2,
  "baseline_tps": 28.1,
  "speedup": 3.35,
  "avg_draft_latency_ms": 18.4,
  "avg_verify_latency_ms": 52.1,
  "overall_acceptance_rate": 0.72
}
```

**Generation Done Event:**
```json
{
  "type": "done",
  "total_tokens": 256,
  "total_accepted": 187,
  "total_drafted": 256,
  "total_rounds": 42,
  "generated_text": "Quantum computing is..."
}
```

---

## ğŸ§ª Error Handling & Robustness

| Layer | Mechanism | Details |
|-------|-----------|---------|
| **WebSocket** | Auto-reconnect | 3-second backoff on disconnect; mounted-state guard prevents leaks |
| **Speculator loop** | Try-catch + ErrorEvent | Any exception is caught, serialized, and sent to frontend |
| **Draft model** | Singleton loading | Loaded once at startup; health endpoint verifies availability |
| **Target model** | API error handling | HTTP errors caught and reported; timeout handling |
| **Frontend state** | StrictMode guards | Deduplication prevents double-processing in development mode |
| **Tokenizer** | Token ID tracking | Eliminates drift from string concatenation across rounds |
| **EOS detection** | Configurable tokens | `Settings.eos_tokens` list (default: Llama + Harmony stop tokens); extensible for new model families |
| **Testing** | Protocol-based DI | `DraftModelProtocol`/`TargetModelProtocol` enable unit tests with stub models â€” no GPU/MLX required |

---

## ğŸ›  Tech Stack

| Layer | Technology | Role |
|-------|-----------|------|
| **Draft inference** | [MLX-LM](https://github.com/ml-explore/mlx-lm) | Apple Silicon optimized LLM inference with Neural Engine acceleration |
| **Target inference** | [Cerebras Cloud](https://cerebras.ai) | Ultra-low latency wafer-scale verification via `/v1/completions` |
| **API client** | [OpenAI Python SDK](https://github.com/openai/openai-python) | Async client for Cerebras-compatible API |
| **Backend framework** | [FastAPI](https://fastapi.tiangolo.com/) | Async Python web framework with WebSocket support |
| **ASGI server** | [Uvicorn](https://www.uvicorn.org/) | High-performance async server with hot reload |
| **Configuration** | [Pydantic Settings](https://docs.pydantic.dev/latest/concepts/pydantic_settings/) | Type-safe environment variable loading with validation |
| **Frontend framework** | [React 19](https://react.dev/) | Component-based UI with hooks and concurrent features |
| **Build tool** | [Vite](https://vitejs.dev/) | Instant HMR, dev proxy for WebSocket + API |
| **Tree visualization** | [D3.js 7](https://d3js.org/) | Force-directed graph with hierarchical tree layout |
| **Charts** | [Recharts 3](https://recharts.org/) | React-native charting (area, pie, bar, composed) |
| **Type safety** | [TypeScript 5.9](https://www.typescriptlang.org/) | End-to-end type safety from WebSocket to components |

---

## ğŸ—º Roadmap

- [ ] **Multi-draft speculative sampling** â€” Run multiple draft models in parallel and select the best speculation
- [ ] **Lookahead decoding** â€” Combine speculative decoding with n-gram lookahead for even deeper speculation
- [ ] **Prompt caching** â€” Cache Cerebras prefix computations across rounds for lower verification latency
- [ ] **Tree attention visualization** â€” Render attention patterns within the token tree
- [ ] **Adaptive K** â€” Dynamically adjust speculation depth based on rolling acceptance rate
- [ ] **Multi-model ensemble** â€” Blend draft distributions from multiple small models
- [ ] **Export & replay** â€” Save generation traces as JSON for offline analysis and presentation

---

## ğŸ“š References

- Leviathan, Y., Kalman, M., & Matias, Y. (2023). [*Fast Inference from Transformers via Speculative Decoding*](https://arxiv.org/abs/2211.17192). ICML 2023.
- Chen, C., et al. (2023). [*Accelerating Large Language Model Decoding with Speculative Sampling*](https://arxiv.org/abs/2302.01318).
- [Cerebras Inference Documentation](https://inference-docs.cerebras.ai/) â€” API reference for wafer-scale inference.
- [MLX-LM Documentation](https://github.com/ml-explore/mlx-lm) â€” Apple's framework for efficient ML on Apple Silicon.

---

<div align="center">

**Built to make the invisible visible.**

Speculative decoding is one of the most impactful inference optimizations available today,
but it's notoriously hard to reason about. SpeculatoViz turns the abstract into the tangible â€”
every draft, every verification, every acceptance and rejection, rendered in real time.

With Cerebras' deterministic, wafer-scale inference powering the verification step,
speculative decoding transitions from a fragile heuristic to a reliable, production-grade speedup.

---

*Questions? Ideas? Let's talk about the future of distributed LLM inference.*

</div>
